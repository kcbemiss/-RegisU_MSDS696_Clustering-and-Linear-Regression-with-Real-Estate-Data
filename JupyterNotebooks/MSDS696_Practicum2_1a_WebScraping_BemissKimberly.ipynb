{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSDS696 Data Science Practicum II\n",
    "### Clustering and Linear Regression with Real Estate Data\n",
    "### Part 1a - Web Scraping a Real Estate Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup #library for webscraping (take care of html)\n",
    "from requests import get #HTTP library for making http requests in python.\n",
    "from time import sleep # use for pauses during code execution\n",
    "from random import randint # use for psudo-random number generation\n",
    "from selenium import webdriver # use for web-scraping\n",
    "#import re # library for regular expressions\n",
    "import regex as re # library for regular expressions\n",
    "import pandas as pd # library for data analysis and manipulation\n",
    "import numpy as np # library for working with arrays\n",
    "import itertools # library that implements iterator building blocks\n",
    "\n",
    "import matplotlib.pyplot as plt # library for ploting data\n",
    "import seaborn as sns # library for plotting data\n",
    "sns.set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web-Scraping Part 1:  Getting a list of property URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The website I will be scraping from is REALTOR.com, searching for properties in Douglas County, Colorado.\n",
    "\n",
    "\"https://www.realtor.com/realestateandhomes-search/Douglas-County_CO\"\n",
    "\n",
    "The search returns 37 pages of results (1,544 Homes), which means that I am going to need to create code that iterates through all 37 pages to collect the html that identifies the URL for each property that can be used in another scrape to collect the data I am interested in. \n",
    "\n",
    "The Steps for part 1 of the web scrape will be:\n",
    "\n",
    "   1) use \"get\" to obtain the HTML code from the website.\n",
    "   \n",
    "   2) use BeautifulSoup and RE to parse the HTML for the property URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "WebDriverException",
     "evalue": "Message: 'geckodriver' executable needs to be in PATH. \n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\common\\service.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[0mcmd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_line_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             self.process = subprocess.Popen(cmd, env=self.env,\n\u001b[0m\u001b[0;32m     73\u001b[0m                                             \u001b[0mclose_fds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplatform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'Windows'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[0;32m    853\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 854\u001b[1;33m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[0;32m    855\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1306\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1307\u001b[1;33m                 hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n\u001b[0m\u001b[0;32m   1308\u001b[0m                                          \u001b[1;31m# no special security\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-19f70832d4bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# use get from the response library to extract the htlm from the first page of the website referenced.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m#response = get(website, headers=headers)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFirefox\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mres_get\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwebsite\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\firefox\\webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, firefox_profile, firefox_binary, timeout, capabilities, proxy, executable_path, options, service_log_path, firefox_options, service_args, desired_capabilities, log_path, keep_alive)\u001b[0m\n\u001b[0;32m    162\u001b[0m                 \u001b[0mservice_args\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mservice_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m                 log_path=service_log_path)\n\u001b[1;32m--> 164\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mservice\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m             \u001b[0mcapabilities\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_capabilities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\common\\service.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mENOENT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                 raise WebDriverException(\n\u001b[0m\u001b[0;32m     82\u001b[0m                     \"'%s' executable needs to be in PATH. %s\" % (\n\u001b[0;32m     83\u001b[0m                         os.path.basename(self.path), self.start_error_message)\n",
      "\u001b[1;31mWebDriverException\u001b[0m: Message: 'geckodriver' executable needs to be in PATH. \n"
     ]
    }
   ],
   "source": [
    "# Be polite when scrapping! - try to not overwhelm the website.\n",
    "\n",
    "# pass the header in the \"get\" command to mimic actual user behavior when web scrapping.  \n",
    "headers = ({'User-Agent':\n",
    "            'Mozilla/5.0 (Linux; U; Android 4.0.4; en-gb; GT-I9300 Build/IMM76D) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Mobile Safari/534.30'}) \n",
    "\n",
    "            #'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36'})\n",
    "\n",
    "# set the location to scrape as the website with the filtered information (first page of results)\n",
    "website = \"https://www.realtor.com/realestateandhomes-search/Douglas-County_CO\"\n",
    "\n",
    "# use get from the response library to extract the htlm from the first page of the website referenced.\n",
    "#response = get(website, headers=headers)\n",
    "response = webdriver.Firefox()\n",
    "res_get = response.get(website)\n",
    "\n",
    "# view the response code\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# View the html returned from the get command.\n",
    "print(response.text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use BeautifulSoup to parse though the html obtained using get.\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Getting the relevant links from the html - identified using \"inspect\" from google chrome on the website\n",
    "# Example:  <a rel=\"noopener\" href=\"/realestateandhomes-detail/9396-Desert-Willow-Rd_Highlands-Ranch_CO_80129_M16601-58901\" target=\"_self\" data-testid=\"property-anchor\"><picture><source srcset=\"https://ap.rdcpix.com/74d2956d9a69906854ddf4fa93ecf3efl-m2340783516od-w480_h360.webp, https://ap.rdcpix.com/74d2956d9a69906854ddf4fa93ecf3efl-m2340783516od-w480_h360_x2.webp 2x\" type=\"image/webp\" data-testid=\"img-webp\"><img alt=\"9396 Desert Willow Rd, Highlands Ranch, CO 80129 with Three Car Garage\" data-src=\"https://ap.rdcpix.com/74d2956d9a69906854ddf4fa93ecf3efl-m2340783516od-w480_h360.jpg\" src=\"https://ap.rdcpix.com/74d2956d9a69906854ddf4fa93ecf3efl-m2340783516od-w480_h360.jpg\" srcset=\"https://ap.rdcpix.com/74d2956d9a69906854ddf4fa93ecf3efl-m2340783516od-w480_h360.jpg, https://ap.rdcpix.com/74d2956d9a69906854ddf4fa93ecf3efl-m2340783516od-w480_h360_x2.jpg 2x\" itemprop=\"image\" class=\"fade bottom\" data-label=\"pc-photo\" data-atf=\"false\" data-fmp=\"false\"></picture><picture><source srcset=\"https://ap.rdcpix.com/74d2956d9a69906854ddf4fa93ecf3efl-m793780717od-w480_h360.webp, https://ap.rdcpix.com/74d2956d9a69906854ddf4fa93ecf3efl-m793780717od-w480_h360_x2.webp 2x\" type=\"image/webp\" data-testid=\"img-webp\"><img alt=\"9396 Desert Willow Rd, Highlands Ranch, CO 80129 with Three Car Garage\" data-src=\"https://ap.rdcpix.com/74d2956d9a69906854ddf4fa93ecf3efl-m793780717od-w480_h360.jpg\" src=\"https://ap.rdcpix.com/74d2956d9a69906854ddf4fa93ecf3efl-m793780717od-w480_h360.jpg\" srcset=\"https://ap.rdcpix.com/74d2956d9a69906854ddf4fa93ecf3efl-m793780717od-w480_h360.jpg, https://ap.rdcpix.com/74d2956d9a69906854ddf4fa93ecf3efl-m793780717od-w480_h360_x2.jpg 2x\" itemprop=\"image\" class=\"fade top\" data-label=\"pc-photo\" data-atf=\"true\" data-fmp=\"false\"></picture></a>\n",
    "\n",
    "links_html = soup.find_all('a', rel = \"noopener\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the object type-\n",
    "type(links_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning the soup into a list of strings using str(x)\n",
    "links_v1 = [str(x) for x in links_html]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the object type-\n",
    "type(links_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the size of the object\n",
    "len(links_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first few items in the list\n",
    "links_v1[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the property URL from the html\n",
    "# Use regex to identify a specific string\n",
    "url = '<a.*href=\"(.*)\"noopener\"'\n",
    "\n",
    "# Use a list comprehension to loop through each item in the list and search for the regex pattern.\n",
    "links_v2a = [re.search(url, x) for x in links_v1]\n",
    "\n",
    "# Use a list comprehension to loop through each item in the list to remove the \"None\" matches (allow for use of .group)\n",
    "links_v2b = [i for i in links_v2a if i]\n",
    "\n",
    "# Use a list comprehension to change each item in the list to a string\n",
    "links_v2c = [str(x) for x in links_v2b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the object type-\n",
    "type(links_v2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the first few items in the list\n",
    "links_v2c[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the property URL from the html\n",
    "\n",
    "pattern = 'href=\"(.*)\".rel=\"noopener\"' #- use with group(1)\n",
    "links_v2d = [re.search(pattern, x).group(1) for x in links_v2c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the object type-\n",
    "type(links_v2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the first few items in the list\n",
    "links_v2d[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The URL needs the website added to the beginning of the string\n",
    "pre_url = 'https://www.realtor.com'\n",
    "pre_url += '% s'\n",
    "web_url =  [pre_url % i for i in links_v2d]\n",
    "web_url[0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the URL's individually to a list (this preps the list for the next step of iterating through multiple web pages)\n",
    "url_list = []\n",
    "\n",
    "for url in web_url:\n",
    "    url_list.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View how many items are in the url list\n",
    "len(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the conent of the URL list\n",
    "url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(url_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up a for loop to iterate through multiple web pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an numpy array of values 1 through 37 to match the number of pages of results from the website\n",
    "pages = np.arange(2, 39)\n",
    "print(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the object type\n",
    "type(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a for loop to iterate through each of the 38 pages.\n",
    "\n",
    "for page in pages:\n",
    "       \n",
    "    url_page_lp = get(\"https://www.realtor.com/realestateandhomes-search/Douglas-County_CO/pg-\" + str(page), headers=headers)\n",
    "    \n",
    "    soup_lp = BeautifulSoup(url_page_lp.content, \"html.parser\")\n",
    "    \n",
    "    links_html_lp = soup_lp.find_all('a', rel = \"noopener\")\n",
    "    \n",
    "    links_v1_lp = [str(x) for x in links_html_lp]\n",
    "    \n",
    "    # Extract the property URL from the html\n",
    "    # Use regex to identify a specific string\n",
    "    url_lp = '<a.*href=\"(.*)\"noopener\"'\n",
    "\n",
    "    # Use a list comprehension to loop through each item in the list and search for the regex pattern.\n",
    "    links_v2a_lp = [re.search(url_lp, x) for x in links_v1_lp]\n",
    "\n",
    "    # Use a list comprehension to loop through each item in the list to remove the \"None\" matches (allow for use of .group)\n",
    "    links_v2b_lp = [i for i in links_v2a_lp if i]\n",
    "\n",
    "    # Use a list comprehension to change each item in the list to a string\n",
    "    links_v2c_lp = [str(x) for x in links_v2b_lp]\n",
    "    \n",
    "    # Extract the property URL from the html\n",
    "\n",
    "    pattern_lp = 'href=\"(.*)\".rel=\"noopener\"' #- use with group(1)\n",
    "    links_v2d_lp = [re.search(pattern_lp, x).group(1) for x in links_v2c_lp]\n",
    "    \n",
    "    pre_url_lp = 'https://www.realtor.com'\n",
    "    pre_url_lp += '% s'\n",
    "    web_url_lp =  [pre_url_lp % i for i in links_v2d_lp]\n",
    "    \n",
    "    for url_item in web_url_lp:\n",
    "        url_list.append(url_item)\n",
    "    \n",
    "    print(page)\n",
    "\n",
    "    \n",
    "    sleep(randint(60,120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the number of items in the list\n",
    "len(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first few items in the list\n",
    "url_list[0:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify if there are any duplicated URLs in the list and create a new list with only unique URLs\n",
    "url_unique = []\n",
    "for i in url_list:\n",
    "    if i not in url_unique:\n",
    "        url_unique.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the number of items in the new list\n",
    "len(url_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the fir\n",
    "url_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REFERENCE\n",
    "\n",
    "###### Realtor.com Robot.txt file\n",
    "https://www.realtor.com/robots.txt\n",
    "\n",
    "###### Webscraping real estate market data\n",
    "https://data4help.medium.com/webscraping-real-estate-market-data-515c0b85b494\n",
    "\n",
    "###### Scraping a real estate website\n",
    "https://towardsdatascience.com/looking-for-a-house-build-a-web-scraper-to-help-you-5ab25badc83e\n",
    "\n",
    "##### Scraping Realtor.com\n",
    "https://www.proxiesapi.com/blog/scraping-listings-from-realtor-with-python-and-bea.html.php\n",
    "\n",
    "###### Scraping multiple pages of a website\n",
    "https://betterprogramming.pub/how-to-scrape-multiple-pages-of-a-website-using-a-python-web-scraper-4e2c641cff8\n",
    "\n",
    "###### How to web scrape, and avoid being tagged as a bot:\n",
    "https://www.scrapehero.com/how-to-prevent-getting-blacklisted-while-scraping/\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
